{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transcript-Conditioned LLM Reasoning Demo\n\nThis notebook demonstrates that a language model does not internally commit to a secret in a 20-questions game. Instead, it reconstructs the secret from the visible transcript.\n\n**Method:** We define a small secret space of animals, generate a unique Q&A transcript for each secret, and show that the same model instance declares a different secret depending solely on the transcript it sees."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install --upgrade filelock Pillow \"numpy>=1.24,<2\" torch torchvision transformers accelerate huggingface_hub bitsandbytes"
  },
  {
   "cell_type": "code",
   "source": "import getpass\nfrom huggingface_hub import login\n\ntoken = getpass.getpass(\"Enter your HuggingFace token: \")\nlogin(token=token)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Secret Space Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SECRETS = {\n    \"cat\": {\n        \"mammal\": True,\n        \"flies\": False,\n        \"swims\": False,\n        \"has_fur\": True,\n        \"fish\": False,\n        \"predator\": True,\n        \"pet\": True,\n        \"barks\": False,\n    },\n    \"dog\": {\n        \"mammal\": True,\n        \"flies\": False,\n        \"swims\": False,\n        \"has_fur\": True,\n        \"fish\": False,\n        \"predator\": True,\n        \"pet\": True,\n        \"barks\": True,\n    },\n    \"eagle\": {\n        \"mammal\": False,\n        \"flies\": True,\n        \"swims\": False,\n        \"has_fur\": False,\n        \"fish\": False,\n        \"predator\": True,\n        \"pet\": False,\n        \"barks\": False,\n    },\n    \"salmon\": {\n        \"mammal\": False,\n        \"flies\": False,\n        \"swims\": True,\n        \"has_fur\": False,\n        \"fish\": True,\n        \"predator\": False,\n        \"pet\": False,\n        \"barks\": False,\n    },\n    \"shark\": {\n        \"mammal\": False,\n        \"flies\": False,\n        \"swims\": True,\n        \"has_fur\": False,\n        \"fish\": True,\n        \"predator\": True,\n        \"pet\": False,\n        \"barks\": False,\n    },\n    \"crocodile\": {\n        \"mammal\": False,\n        \"flies\": False,\n        \"swims\": True,\n        \"has_fur\": False,\n        \"fish\": False,\n        \"predator\": True,\n        \"pet\": False,\n        \"barks\": False,\n    },\n}\n\n# Verify all secrets have the same feature keys\nfeature_keys = list(next(iter(SECRETS.values())).keys())\nassert all(list(v.keys()) == feature_keys for v in SECRETS.values()), \"Feature keys mismatch\"\n\n# Verify all secrets are unique\nvectors = [tuple(v.values()) for v in SECRETS.values()]\nassert len(vectors) == len(set(vectors)), \"Duplicate feature vectors found\"\n\nprint(f\"Defined {len(SECRETS)} secrets with {len(feature_keys)} features each.\")\nprint(f\"Features: {feature_keys}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "QUESTIONS = {\n    \"mammal\": \"Is it a mammal?\",\n    \"flies\": \"Does it fly?\",\n    \"swims\": \"Does it swim?\",\n    \"has_fur\": \"Does it have fur?\",\n    \"fish\": \"Is it a fish?\",\n    \"predator\": \"Is it a predator?\",\n    \"pet\": \"Is it commonly kept as a pet?\",\n    \"barks\": \"Does it bark?\",\n}\n\n# Verify 1-to-1 mapping with feature keys\nassert set(QUESTIONS.keys()) == set(feature_keys), \"Questions don't match features\"\nprint(\"Question templates defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transcript Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_transcript(secret_name: str) -> str:\n    \"\"\"Build a Q&A transcript for the given secret using fixed question order.\"\"\"\n    features = SECRETS[secret_name]\n    lines = []\n    for feature in feature_keys:\n        question = QUESTIONS[feature]\n        answer = \"Yes.\" if features[feature] else \"No.\"\n        lines.append(f\"User: {question}\")\n        lines.append(f\"Assistant: {answer}\")\n    return \"\\n\".join(lines)\n\n\n# Preview one transcript\nprint(\"=== Transcript for 'eagle' ===\")\nprint(build_transcript(\"eagle\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SECRET_LIST = list(SECRETS.keys())\n\n\ndef build_prompt(secret_list: list[str], transcript: str) -> str:\n    \"\"\"Wrap transcript into a prompt for the model.\"\"\"\n    secrets_str = \", \".join(secret_list)\n    return (\n        f\"You are playing 20 questions. You have secretly chosen one of the following: \"\n        f\"{secrets_str}.\\n\"\n        f\"Here is the conversation so far:\\n\\n\"\n        f\"{transcript}\\n\\n\"\n        f\"User: I give up. What was the secret?\\n\"\n        f\"Assistant:\"\n    )\n\n\n# Preview\nprint(build_prompt(SECRET_LIST, build_transcript(\"shark\")))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feasibility Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feasible_set(transcript: str) -> list[str]:\n",
    "    \"\"\"Parse transcript answers and return secrets consistent with them.\"\"\"\n",
    "    # Parse answers from transcript\n",
    "    observed = {}\n",
    "    lines = transcript.strip().split(\"\\n\")\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:\n",
    "        user_line = lines[i].strip()\n",
    "        assistant_line = lines[i + 1].strip()\n",
    "        if user_line.startswith(\"User:\") and assistant_line.startswith(\"Assistant:\"):\n",
    "            question_text = user_line[len(\"User:\"):].strip()\n",
    "            answer_text = assistant_line[len(\"Assistant:\"):].strip().lower()\n",
    "            # Find which feature this question maps to\n",
    "            for feature, q in QUESTIONS.items():\n",
    "                if q == question_text:\n",
    "                    observed[feature] = answer_text.startswith(\"yes\")\n",
    "                    break\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Filter secrets\n",
    "    feasible = []\n",
    "    for name, features in SECRETS.items():\n",
    "        match = True\n",
    "        for feature, value in observed.items():\n",
    "            if features.get(feature) != value:\n",
    "                match = False\n",
    "                break\n",
    "        if match:\n",
    "            feasible.append(name)\n",
    "    return feasible\n",
    "\n",
    "\n",
    "# Verify every secret's transcript uniquely identifies it\n",
    "for name in SECRETS:\n",
    "    t = build_transcript(name)\n",
    "    fs = compute_feasible_set(t)\n",
    "    assert fs == [name], f\"Expected [{name}], got {fs}\"\n",
    "    print(f\"{name}: feasible set = {fs}  ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import BitsAndBytesConfig\n\nMODEL_NAME = \"meta-llama/Llama-3.1-70B-Instruct\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nprint(f\"Loading tokenizer from {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nprint(f\"Loading model from {MODEL_NAME} (4-bit quantized)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    quantization_config=quantization_config,\n)\nmodel.eval()\n\nprint(\"Model loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(transcript: str) -> str:\n",
    "    \"\"\"Run inference on a transcript and return the model's predicted secret.\"\"\"\n",
    "    prompt = build_prompt(SECRET_LIST, transcript)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens\n",
    "    generated = tokenizer.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all transcripts\n",
    "transcripts = {name: build_transcript(name) for name in SECRETS}\n",
    "\n",
    "# Run experiments\n",
    "results = {}\n",
    "for name, transcript in transcripts.items():\n",
    "    prediction = run_experiment(transcript)\n",
    "    feasible = compute_feasible_set(transcript)\n",
    "    results[name] = prediction\n",
    "\n",
    "    print(f\"Transcript implied: {name}\")\n",
    "    print(f\"Feasible set size: {len(feasible)}\")\n",
    "    print(f\"Feasible set: {feasible}\")\n",
    "    print(f\"Model said: {prediction}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Contradictory Case\n",
    "\n",
    "This transcript is logically impossible — no secret matches all answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "contradictory_transcript = \"\"\"User: Is it a mammal?\nAssistant: Yes.\nUser: Is it a fish?\nAssistant: Yes.\nUser: Does it fly?\nAssistant: Yes.\"\"\"\n\nfeasible = compute_feasible_set(contradictory_transcript)\nprint(f\"Feasible set size: {len(feasible)}\")\nprint(f\"Feasible set: {feasible}\")\nprint()\n\ncontradictory_result = run_experiment(contradictory_transcript)\nprint(f\"Contradictory transcript result:\")\nprint(f\"Model said: {contradictory_result}\")\nprint(\"-\" * 40)\nprint()\nprint(\"The model produced an answer despite no valid secret existing.\")\nprint(\"This confirms the model reconstructs state from the transcript\")\nprint(\"rather than maintaining a committed hidden state.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "# Check if different transcripts produced different outputs\n",
    "unique_outputs = set(results.values())\n",
    "print(f\"Unique model outputs: {len(unique_outputs)} (from {len(results)} transcripts)\")\n",
    "print()\n",
    "\n",
    "for name, prediction in results.items():\n",
    "    match_marker = \"\" if name.lower() in prediction.lower() else \"  <-- MISMATCH\"\n",
    "    print(f\"  {name:>12s} -> {prediction}{match_marker}\")\n",
    "\n",
    "print()\n",
    "print(f\"Contradictory -> {contradictory_result}\")\n",
    "print()\n",
    "print(\"If most consistent transcripts yielded the correct secret,\")\n",
    "print(\"the model is reconstructing state from the transcript.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Reproducibility Info\n\nThe results above were produced in the following environment. The cell below will dynamically capture the current environment when re-run; the reference configuration is recorded here.\n\n| Component      | Value                                  |\n|----------------|----------------------------------------|\n| Platform       | Linux 6.8.0-1040-nvidia-64k (Ubuntu 22.04.5 LTS) |\n| Architecture   | aarch64                                |\n| Python         | 3.10.12                                |\n| PyTorch        | 2.7.0                                  |\n| Transformers   | 5.1.0                                  |\n| Accelerate     | 1.12.0                                 |\n| Bitsandbytes   | 0.49.1                                 |\n| NumPy          | ≥1.24, <2 (pinned)                    |\n| GPU            | NVIDIA GH200 480GB (97,871 MiB)        |\n| Model          | meta-llama/Llama-3.1-70B-Instruct      |\n| Quantization   | 4-bit (BitsAndBytesConfig, fp16 compute) |\n| Temperature    | 0.1                                    |\n| max_new_tokens | 50                                     |\n| do_sample      | True                                   |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import platform\nimport subprocess\nimport transformers\nimport bitsandbytes\nimport accelerate\n\ndef get_gpu_info():\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n            capture_output=True, text=True\n        )\n        return result.stdout.strip()\n    except FileNotFoundError:\n        return \"nvidia-smi not found\"\n\nprint(\"=\" * 50)\nprint(\"REPRODUCIBILITY INFO\")\nprint(\"=\" * 50)\nprint()\nprint(f\"Platform:        {platform.platform()}\")\nprint(f\"Architecture:    {platform.machine()}\")\nprint(f\"Python:          {platform.python_version()}\")\nprint(f\"PyTorch:         {torch.__version__}\")\nprint(f\"CUDA available:  {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version:    {torch.version.cuda}\")\nprint(f\"Transformers:    {transformers.__version__}\")\nprint(f\"Accelerate:      {accelerate.__version__}\")\nprint(f\"Bitsandbytes:    {bitsandbytes.__version__}\")\nprint(f\"NumPy:           {__import__('numpy').__version__}\")\nprint()\nprint(f\"GPU:             {get_gpu_info()}\")\nprint()\nprint(f\"Model:           {MODEL_NAME}\")\nprint(f\"Quantization:    4-bit (BitsAndBytesConfig)\")\nprint(f\"Temperature:     0.1\")\nprint(f\"max_new_tokens:  50\")\nprint(f\"do_sample:       True\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}